{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6288458a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yehan Perera\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ydata_profiling import ProfileReport\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from ydata_profiling import ProfileReport\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ydata_profiling import ProfileReport\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from warnings import filterwarnings\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, silhouette_samples\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from itertools import combinations\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "### so that u dont have warnings\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1e61fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"zomato.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11ae5a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_initial(data):\n",
    "    ### Remove unwanted columns \n",
    "    col_to_remove = ['url','address','phone','reviews_list','menu_item']\n",
    "    df = data.drop(columns=col_to_remove)\n",
    "    \n",
    "    # remove duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "\n",
    "    def fun(x):\n",
    "        x_new = x.split(',')\n",
    "        return x_new[0]+x_new[1]\n",
    "    df['approx_cost(for two people)']=df['approx_cost(for two people)'].apply(lambda x: fun(x) if ',' in str(x) else x)\n",
    "    df['approx_cost(for two people)'] = pd.to_numeric(df['approx_cost(for two people)'])\n",
    "    \n",
    "    \n",
    "    # rate \n",
    "    df['rate'] = df['rate'].apply(lambda x : str(x).split('/')[0])\n",
    "    ls = [float(i) for i in df['rate'] if (i!='NEW') & (i!='nan') & (i!='-')]\n",
    "    mean_rate = round(np.mean(ls),1)\n",
    "    df['rate'] = df['rate'].apply(lambda x: mean_rate if (x=='NEW') | (x=='-') | (x=='nan') else x)\n",
    "    df['rate'] = pd.to_numeric(df['rate'])\n",
    "    ## Remove the missing values of response \n",
    "    df.dropna(subset=['approx_cost(for two people)'], inplace=True)\n",
    "    \n",
    "     #########   categorical variables \n",
    "    vars_to_impute = ['location','rest_type','dish_liked','cuisines']\n",
    "    for col in vars_to_impute:\n",
    "        df.dropna(subset=[col],inplace=True)\n",
    "        \n",
    "        \n",
    "    ## Split the data \n",
    "    X= df.drop(columns=['approx_cost(for two people)'])\n",
    "    Y = df['approx_cost(for two people)']\n",
    "   \n",
    "    return   X ,Y \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e18aa24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X , Y = preprocessing_initial(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b062b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01bbaec4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Mean Absolute Percentage Error on Training set: 15.863344466995214\n",
      "Mean Absolute Percentage Error on Testing set: 15.887036818855426\n",
      "Root Mean Squared Error on Training set: 270.35320753553026\n",
      "Root Mean Squared Error on Testing set: 286.5207343047687\n",
      "{'elastic_net__alpha': 0.1, 'elastic_net__l1_ratio': 0.9, 'feature_selection__estimator__alpha': 10.0, 'feature_selection__estimator__l1_ratio': 0.1}\n",
      "Mean Absolute Percentage Error on Training set (Cluster 2): 25.661081729540534\n",
      "Mean Absolute Percentage Error on Testing set (Cluster 2): 25.849990211506235\n",
      "Root Mean Squared Error on Training set (Cluster 2): 140.84603463145217\n",
      "Root Mean Squared Error on Testing set (Cluster 2): 144.79773035300232\n",
      "{'elastic_net__alpha': 10.0, 'elastic_net__l1_ratio': 0.1, 'feature_selection__estimator__alpha': 10.0, 'feature_selection__estimator__l1_ratio': 0.5}\n"
     ]
    }
   ],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Define your target encoder columns\n",
    "target_encoder_cols = ['name', 'dish_liked', 'cuisines', 'location', 'rest_type', 'listed_in(city)', 'listed_in(type)', 'online_order', 'book_table']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform frequency encoding for each column\n",
    "for col in target_encoder_cols:\n",
    "    target_encoder = TargetEncoder()\n",
    "\n",
    "    # Fit and transform the target encoder on the training data\n",
    "    X_train[target_encoder_cols] = target_encoder.fit_transform(X_train[target_encoder_cols], y_train)\n",
    "\n",
    "    # Transform the test data using the target encoder fitted on the training data\n",
    "    X_test[target_encoder_cols] = target_encoder.fit_transform(X_test[target_encoder_cols],y_test)\n",
    "    \n",
    "    \n",
    "from sklearn.ensemble import IsolationForest\n",
    "random_seed = 42\n",
    "clf = IsolationForest(contamination=0.2, random_state=random_seed)\n",
    "clf.fit(X_train)\n",
    "predictions = clf.predict(X_train)\n",
    "\n",
    "X_train['cluster_label'] = predictions\n",
    "\n",
    "X_train_1 = X_train[X_train['cluster_label']== -1]\n",
    "X_train_2 = X_train[X_train['cluster_label']== 1]\n",
    "\n",
    "indexes_1 = X_train_1.index\n",
    "indexes_2 = X_train_2.index\n",
    "\n",
    "\n",
    "y_train_1 = y_train.loc[indexes_1]\n",
    "y_train_2 = y_train.loc[indexes_2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train_cluster_1 , x_test_cluster_1 , y_train_cluster_1 , y_test_cluster_1 = train_test_split(X_train_1,y_train_1,test_size=0.2,random_state=42)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create Pipeline with feature selection and ElasticNet\n",
    "pipeline = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(ElasticNet())),\n",
    "    ('elastic_net', ElasticNet())\n",
    "])\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'feature_selection__estimator__alpha': [0.1, 1.0, 10.0],\n",
    "    'feature_selection__estimator__l1_ratio': [0.1, 0.5, 0.9],\n",
    "    'elastic_net__alpha': [0.1, 1.0, 10.0],\n",
    "    'elastic_net__l1_ratio': [0.1, 0.5, 0.9]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(x_train_cluster_1, y_train_cluster_1)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "selected_features_1 = x_train_cluster_1.columns[best_model.named_steps['feature_selection'].get_support()]\n",
    "\n",
    "# Predictions for Cluster 1\n",
    "y_train_pred_cluster_1 = best_model.predict(x_train_cluster_1)\n",
    "y_test_pred_cluster_1 = best_model.predict(x_test_cluster_1)\n",
    "\n",
    "# Calculate RMSE for Cluster 1\n",
    "rmse_train_cluster_1 = np.sqrt(mean_squared_error(y_train_cluster_1, y_train_pred_cluster_1))\n",
    "rmse_test_cluster_1 = np.sqrt(mean_squared_error(y_test_cluster_1, y_test_pred_cluster_1))\n",
    "\n",
    "# Evaluate the best model\n",
    "mape_train_cluster_1 = np.mean(np.abs((y_train_cluster_1 - y_train_pred_cluster_1) / y_train_cluster_1)) * 100\n",
    "mape_test_cluster_1 = np.mean(np.abs((y_test_cluster_1 - y_test_pred_cluster_1) / y_test_cluster_1)) * 100\n",
    "\n",
    "print(\"Mean Absolute Percentage Error on Training set:\", mape_train_cluster_1)\n",
    "print(\"Mean Absolute Percentage Error on Testing set:\", mape_test_cluster_1)\n",
    "print(\"Root Mean Squared Error on Training set:\", rmse_train_cluster_1)\n",
    "print(\"Root Mean Squared Error on Testing set:\", rmse_test_cluster_1)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train_cluster_2 , x_test_cluster_2 , y_train_cluster_2 , y_test_cluster_2 = train_test_split(X_train_2,\n",
    "                                                                                    y_train_2,test_size=0.2,random_state=42)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create Pipeline with feature selection and ElasticNet\n",
    "pipeline = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(ElasticNet())),\n",
    "    ('elastic_net', ElasticNet())\n",
    "])\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'feature_selection__estimator__alpha': [0.1, 1.0, 10.0],\n",
    "    'feature_selection__estimator__l1_ratio': [0.1, 0.5, 0.9],\n",
    "    'elastic_net__alpha': [0.1, 1.0, 10.0],\n",
    "    'elastic_net__l1_ratio': [0.1, 0.5, 0.9]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(x_train_cluster_2, y_train_cluster_2)\n",
    "\n",
    "# Get the best model\n",
    "best_model_2 = grid_search.best_estimator_\n",
    "selected_features_2 = x_train_cluster_2.columns[best_model_2.named_steps['feature_selection'].get_support()]\n",
    "\n",
    "# Predictions for Cluster 2\n",
    "y_train_pred_cluster_2 = best_model_2.predict(x_train_cluster_2)\n",
    "y_test_pred_cluster_2 = best_model_2.predict(x_test_cluster_2)\n",
    "\n",
    "# Calculate RMSE for Cluster 2\n",
    "rmse_train_cluster_2 = np.sqrt(mean_squared_error(y_train_cluster_2, y_train_pred_cluster_2))\n",
    "rmse_test_cluster_2 = np.sqrt(mean_squared_error(y_test_cluster_2, y_test_pred_cluster_2))\n",
    "\n",
    "# Evaluate the best model\n",
    "mape_train_cluster_2 = np.mean(np.abs((y_train_cluster_2 - y_train_pred_cluster_2) / y_train_cluster_2)) * 100\n",
    "mape_test_cluster_2 = np.mean(np.abs((y_test_cluster_2 - y_test_pred_cluster_2) / y_test_cluster_2)) * 100\n",
    "\n",
    "print(\"Mean Absolute Percentage Error on Training set (Cluster 2):\", mape_train_cluster_2)\n",
    "print(\"Mean Absolute Percentage Error on Testing set (Cluster 2):\", mape_test_cluster_2)\n",
    "print(\"Root Mean Squared Error on Training set (Cluster 2):\", rmse_train_cluster_2)\n",
    "print(\"Root Mean Squared Error on Testing set (Cluster 2):\", rmse_test_cluster_2)\n",
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46327891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd2ed5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93fc573f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error for Cluster 1: 19.935773374097337\n",
      "Mean Absolute Percentage Error for Cluster 2: 29.978148291954504\n"
     ]
    }
   ],
   "source": [
    "new_preds = clf.predict(X_test)\n",
    "\n",
    "# Define best models for each cluster\n",
    "best_model_cluster_1 = best_model\n",
    "best_model_cluster_2 = best_model_2\n",
    "\n",
    "# Assign cluster labels to X_test\n",
    "X_test['cluster_label'] = new_preds\n",
    "\n",
    "# Filter X_test for each cluster\n",
    "X_test_1 = X_test[X_test['cluster_label'] == -1]\n",
    "X_test_2 = X_test[X_test['cluster_label'] == 1]\n",
    "\n",
    "# Get the indexes for each cluster\n",
    "indexes_1 = X_test_1.index\n",
    "indexes_2 = X_test_2.index\n",
    "\n",
    "# Filter y_test for each cluster\n",
    "y_test_1 = y_test.loc[indexes_1]\n",
    "y_test_2 = y_test.loc[indexes_2]\n",
    "\n",
    "# Predictions for each cluster using the corresponding best model\n",
    "y_test_pred_cluster_1 = best_model_cluster_1.predict(X_test_1)\n",
    "y_test_pred_cluster_2 = best_model_cluster_2.predict(X_test_2)\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE) for cluster 1\n",
    "mape_cluster_1 = np.mean(np.abs((y_test_1 - y_test_pred_cluster_1) / y_test_1)) * 100\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE) for cluster 2\n",
    "mape_cluster_2 = np.mean(np.abs((y_test_2 - y_test_pred_cluster_2) / y_test_2)) * 100\n",
    "\n",
    "# Print MAPE for each cluster\n",
    "print(\"Mean Absolute Percentage Error for Cluster 1:\", mape_cluster_1)\n",
    "print(\"Mean Absolute Percentage Error for Cluster 2:\", mape_cluster_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a671316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "795f86c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for Cluster 1: 178851.14812516596\n",
      "Root Mean Squared Error for Cluster 1: 422.9079664952718\n",
      "Mean Squared Error for Cluster 2: 27015.89207150536\n",
      "Root Mean Squared Error for Cluster 2: 164.36511817142153\n"
     ]
    }
   ],
   "source": [
    "# Calculate Mean Squared Error (MSE) for cluster 1\n",
    "mse_cluster_1 = mean_squared_error(y_test_1, y_test_pred_cluster_1)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for cluster 2\n",
    "mse_cluster_2 = mean_squared_error(y_test_2, y_test_pred_cluster_2)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for cluster 1\n",
    "rmse_cluster_1 = math.sqrt(mse_cluster_1)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for cluster 2\n",
    "rmse_cluster_2 = math.sqrt(mse_cluster_2)\n",
    "\n",
    "# Print MSE and RMSE for each cluster\n",
    "print(\"Mean Squared Error for Cluster 1:\", mse_cluster_1)\n",
    "print(\"Root Mean Squared Error for Cluster 1:\", rmse_cluster_1)\n",
    "print(\"Mean Squared Error for Cluster 2:\", mse_cluster_2)\n",
    "print(\"Root Mean Squared Error for Cluster 2:\", rmse_cluster_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6addc183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5183bf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error for Combined Predictions: 28.473616379704474\n"
     ]
    }
   ],
   "source": [
    "# Combine predictions and actual values for both clusters\n",
    "y_test_combined = pd.concat([y_test_1, y_test_2], axis=0)\n",
    "\n",
    "# Combine predictions for both clusters\n",
    "y_pred_combined = np.concatenate([y_test_pred_cluster_1, y_test_pred_cluster_2])\n",
    "\n",
    "# Calculate MAPE for combined predictions\n",
    "mape_combined = np.mean(np.abs((y_test_combined - y_pred_combined) / y_test_combined)) * 100\n",
    "\n",
    "# Print MAPE for combined predictions\n",
    "print(\"Mean Absolute Percentage Error for Combined Predictions:\", mape_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97b96145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for Combined Predictions: 49763.59766962805\n",
      "Root Mean Squared Error for Combined Predictions: 223.07755976258133\n"
     ]
    }
   ],
   "source": [
    "# Calculate Mean Squared Error (MSE) for combined predictions\n",
    "mse_combined = mean_squared_error(y_test_combined, y_pred_combined)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for combined predictions\n",
    "rmse_combined = math.sqrt(mse_combined)\n",
    "\n",
    "# Print MSE and RMSE for combined predictions\n",
    "print(\"Mean Squared Error for Combined Predictions:\", mse_combined)\n",
    "print(\"Root Mean Squared Error for Combined Predictions:\", rmse_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff49a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c07280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3aa69f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8510b9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0847d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aadf0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f3c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec5a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93bda8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b7aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69f7386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad62fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78afb18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1bd466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19acb327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46305e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7cb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e8b519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5dcd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579b4e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbdc6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e9778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd51f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b094d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a1075c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454aa1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
