{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a308f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yehan Perera\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ydata_profiling import ProfileReport\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from ydata_profiling import ProfileReport\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ydata_profiling import ProfileReport\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from warnings import filterwarnings\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, silhouette_samples\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from itertools import combinations\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "### so that u dont have warnings\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59188345",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"zomato.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d0602",
   "metadata": {},
   "source": [
    "## Capture the categories that helps to predict y more accurately "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f86da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_initial(data):\n",
    "    ### Remove unwanted columns \n",
    "    col_to_remove = ['url','address','phone','reviews_list','menu_item']\n",
    "    df = data.drop(columns=col_to_remove)\n",
    "    \n",
    "    # remove duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "\n",
    "    def fun(x):\n",
    "        x_new = x.split(',')\n",
    "        return x_new[0]+x_new[1]\n",
    "    df['approx_cost(for two people)']=df['approx_cost(for two people)'].apply(lambda x: fun(x) if ',' in str(x) else x)\n",
    "    df['approx_cost(for two people)'] = pd.to_numeric(df['approx_cost(for two people)'])\n",
    "    \n",
    "    \n",
    "    # rate \n",
    "    df['rate'] = df['rate'].apply(lambda x : str(x).split('/')[0])\n",
    "    ls = [float(i) for i in df['rate'] if (i!='NEW') & (i!='nan') & (i!='-')]\n",
    "    mean_rate = round(np.mean(ls),1)\n",
    "    df['rate'] = df['rate'].apply(lambda x: mean_rate if (x=='NEW') | (x=='-') | (x=='nan') else x)\n",
    "    df['rate'] = pd.to_numeric(df['rate'])\n",
    "    ## Remove the missing values of response \n",
    "    df.dropna(subset=['approx_cost(for two people)'], inplace=True)\n",
    "    \n",
    "     #########   categorical variables \n",
    "    vars_to_impute = ['location','rest_type','dish_liked','cuisines']\n",
    "    for col in vars_to_impute:\n",
    "        df.dropna(subset=[col],inplace=True)\n",
    "        \n",
    "        \n",
    "    ## Split the data \n",
    "    X= df.drop(columns=['approx_cost(for two people)'])\n",
    "    Y = df['approx_cost(for two people)']\n",
    "   \n",
    "    return   X ,Y \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3165418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X , Y = preprocessing_initial(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "036adf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aecef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(X,Y,test_size=0.2,random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f44aa1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Define your target encoder columns\n",
    "target_encoder_cols = ['name', 'dish_liked', 'cuisines', 'location', 'rest_type', 'listed_in(city)', 'listed_in(type)', 'online_order', 'book_table']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform frequency encoding for each column\n",
    "for col in target_encoder_cols:\n",
    "    target_encoder = TargetEncoder()\n",
    "\n",
    "    # Fit and transform the target encoder on the training data\n",
    "    X_train[target_encoder_cols] = target_encoder.fit_transform(X_train[target_encoder_cols], y_train)\n",
    "\n",
    "    # Transform the test data using the target encoder fitted on the training data\n",
    "    X_test[target_encoder_cols] = target_encoder.transform(X_test[target_encoder_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b71681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0575b614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f5e934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e24a07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1329204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3448b315",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = IsolationForest(contamination=0.1)\n",
    "clf.fit(X_train)\n",
    "predictions = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd27124b",
   "metadata": {},
   "source": [
    "* predictions==1 : cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5f9cb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16842"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predictions==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adffd48c",
   "metadata": {},
   "source": [
    "* predictions==-1 : outliers : cluster 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a0b19a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1872"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predictions==-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a58f165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['cluster_label'] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae86052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[X_train['cluster_label']== 1]\n",
    "indexes_1 = X_train.index\n",
    "y_train = y_train.loc[indexes_1]\n",
    "X_train.drop(columns=['cluster_label'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8990eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc1ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e517d7ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24796d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error on Training set: 24.889517373175345\n",
      "Mean Absolute Percentage Error on Testing set: 27.869743076525637\n",
      "{'elastic_net__alpha': 0.1, 'elastic_net__l1_ratio': 0.9, 'feature_selection__estimator__alpha': 1.0, 'feature_selection__estimator__l1_ratio': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create Pipeline with feature selection and ElasticNet\n",
    "pipeline = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(ElasticNet())),\n",
    "    ('elastic_net', ElasticNet())\n",
    "])\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'feature_selection__estimator__alpha': [0.1, 1.0, 10.0],\n",
    "    'feature_selection__estimator__l1_ratio': [0.1, 0.5, 0.9],\n",
    "    'elastic_net__alpha': [0.1, 1.0, 10.0],\n",
    "    'elastic_net__l1_ratio': [0.1, 0.5, 0.9]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "selected_features_1 = X_train.columns[best_model.named_steps['feature_selection'].get_support()]\n",
    "\n",
    "\n",
    "selected_features = X_train.columns[best_model.named_steps['feature_selection'].get_support()]\n",
    "# Evaluate the best model\n",
    "mape_train = np.mean(np.abs((y_train - best_model.predict(X_train)) / y_train)) * 100\n",
    "mape_test = np.mean(np.abs((y_test - best_model.predict(X_test)) / y_test)) * 100\n",
    "\n",
    "print(\"Mean Absolute Percentage Error on Training set:\", mape_train)\n",
    "print(\"Mean Absolute Percentage Error on Testing set:\", mape_test)\n",
    "\n",
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe98e404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'rate', 'dish_liked'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df97dd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Training set: 22922.780359754823\n",
      "Mean Squared Error on Testing set: 55611.81193163657\n",
      "Root Mean Squared Error on Training set: 151.402709221978\n",
      "Root Mean Squared Error on Testing set: 235.82156799503426\n"
     ]
    }
   ],
   "source": [
    "# Calculate predictions for both training and testing sets\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "# Print MSE and RMSE for both training and testing sets\n",
    "print(\"Mean Squared Error on Training set:\", mse_train)\n",
    "print(\"Mean Squared Error on Testing set:\", mse_test)\n",
    "print(\"Root Mean Squared Error on Training set:\", rmse_train)\n",
    "print(\"Root Mean Squared Error on Testing set:\", rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff69ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42693762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178711a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08b500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "675122f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error on Training set: 24.903982949359428\n",
      "Mean Absolute Percentage Error on Testing set: 27.883032816721325\n",
      "{'feature_selection__estimator__alpha': 0.1, 'feature_selection__estimator__solver': 'sag', 'ridge__alpha': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge  # Change this import\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create Pipeline with feature selection and Ridge regression\n",
    "pipeline = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(Ridge())),  # Change SelectFromModel\n",
    "    ('ridge', Ridge())  # Change Ridge\n",
    "])\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'feature_selection__estimator__alpha': [0.1, 1.0, 10.0],\n",
    "    'feature_selection__estimator__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],  # Add solver options\n",
    "    'ridge__alpha': [0.1, 1.0, 10.0]  # Change parameter name\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "selected_features_1 = X_train.columns[best_model.named_steps['feature_selection'].get_support()]\n",
    "\n",
    "# Evaluate the best model\n",
    "mape_train = np.mean(np.abs((y_train - best_model.predict(X_train)) / y_train)) * 100\n",
    "mape_test = np.mean(np.abs((y_test - best_model.predict(X_test)) / y_test)) * 100\n",
    "\n",
    "print(\"Mean Absolute Percentage Error on Training set:\", mape_train)\n",
    "print(\"Mean Absolute Percentage Error on Testing set:\", mape_test)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90724b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Training set: 22921.280288173413\n",
      "Mean Squared Error on Testing set: 55613.65446565868\n",
      "Root Mean Squared Error on Training set: 151.3977552283171\n",
      "Root Mean Squared Error on Testing set: 235.8254745901271\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate predictions for both training and testing sets\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "# Print MSE and RMSE for both training and testing sets\n",
    "print(\"Mean Squared Error on Training set:\", mse_train)\n",
    "print(\"Mean Squared Error on Testing set:\", mse_test)\n",
    "print(\"Root Mean Squared Error on Training set:\", rmse_train)\n",
    "print(\"Root Mean Squared Error on Testing set:\", rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bd699a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f7ba81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e81eae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (XGBoost): Index(['dish_liked'], dtype='object')\n",
      "Mean Absolute Percentage Error on Training set (XGBoost): 8.194567865722085\n",
      "Mean Absolute Percentage Error on Testing set (XGBoost): 14.519004442554845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Create Pipeline with feature selection and XGBoost\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(XGBRegressor())),\n",
    "    ('xgb', XGBRegressor())\n",
    "])\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'feature_selection__estimator__alpha': [0.1, 1.0, 10.0],\n",
    "    'feature_selection__estimator__max_iter': [1000, 5000, 10000],\n",
    "    'xgb__learning_rate': [0.1, 0.01, 0.001],\n",
    "    'xgb__n_estimators': [100, 500, 1000],\n",
    "    'xgb__max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for XGBoost\n",
    "grid_search_xgb = GridSearchCV(pipeline_xgb, param_grid_xgb, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV for XGBoost\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model for XGBoost\n",
    "best_model_xgb_2 = grid_search_xgb.best_estimator_\n",
    "\n",
    "# Get the selected features for XGBoost\n",
    "selected_features_xgb = X_train.columns[best_model_xgb_2.named_steps['feature_selection'].get_support()]\n",
    "\n",
    "# Evaluate the best model for XGBoost\n",
    "mape_train_xgb = np.mean(np.abs((y_train - best_model_xgb_2.predict(X_train)) / y_train)) * 100\n",
    "mape_test_xgb = np.mean(np.abs((y_test - best_model_xgb_2.predict(X_test)) / y_test)) * 100\n",
    "\n",
    "print(\"Selected Features (XGBoost):\", selected_features_xgb)\n",
    "print(\"Mean Absolute Percentage Error on Training set (XGBoost):\", mape_train_xgb)\n",
    "print(\"Mean Absolute Percentage Error on Testing set (XGBoost):\", mape_test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "304bf2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Training set (XGBoost): 8703.028534778436\n",
      "Mean Squared Error on Testing set (XGBoost): 93422.37152354191\n",
      "Root Mean Squared Error on Training set (XGBoost): 93.29002376877409\n",
      "Root Mean Squared Error on Testing set (XGBoost): 305.650734537874\n"
     ]
    }
   ],
   "source": [
    "y_train_pred_xgb = best_model_xgb_2.predict(X_train)\n",
    "y_test_pred_xgb = best_model_xgb_2.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse_train_xgb = mean_squared_error(y_train, y_train_pred_xgb)\n",
    "mse_test_xgb = mean_squared_error(y_test, y_test_pred_xgb)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse_train_xgb = np.sqrt(mse_train_xgb)\n",
    "rmse_test_xgb = np.sqrt(mse_test_xgb)\n",
    "\n",
    "# Print MSE and RMSE for both training and testing sets\n",
    "print(\"Mean Squared Error on Training set (XGBoost):\", mse_train_xgb)\n",
    "print(\"Mean Squared Error on Testing set (XGBoost):\", mse_test_xgb)\n",
    "print(\"Root Mean Squared Error on Training set (XGBoost):\", rmse_train_xgb)\n",
    "print(\"Root Mean Squared Error on Testing set (XGBoost):\", rmse_test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ae5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cad7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2b94e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error on Training set: 20.963511293986027\n",
      "Mean Absolute Percentage Error on Testing set: 22.843398049195308\n",
      "{'feature_selection__estimator__alpha': 0.1, 'lasso__alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create Pipeline with feature selection and Lasso regression\n",
    "pipeline = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(Lasso())),\n",
    "    ('lasso', Lasso())\n",
    "])\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'feature_selection__estimator__alpha': [0.1, 1.0, 10.0],\n",
    "    'lasso__alpha': [0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "selected_features = X_train.columns[best_model.named_steps['feature_selection'].get_support()]\n",
    "\n",
    "# Evaluate the best model\n",
    "mape_train = np.mean(np.abs((y_train - best_model.predict(X_train)) / y_train)) * 100\n",
    "mape_test = np.mean(np.abs((y_test - best_model.predict(X_test)) / y_test)) * 100\n",
    "\n",
    "print(\"Mean Absolute Percentage Error on Training set:\", mape_train)\n",
    "print(\"Mean Absolute Percentage Error on Testing set:\", mape_test)\n",
    "\n",
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "372ed1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Training set: 17861.527375804737\n",
      "Mean Squared Error on Testing set: 38599.439189848\n",
      "Root Mean Squared Error on Training set: 133.64702531595955\n",
      "Root Mean Squared Error on Testing set: 196.4673998144425\n"
     ]
    }
   ],
   "source": [
    "# Calculate predictions for both training and testing sets\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "# Print MSE and RMSE for both training and testing sets\n",
    "print(\"Mean Squared Error on Training set:\", mse_train)\n",
    "print(\"Mean Squared Error on Testing set:\", mse_test)\n",
    "print(\"Root Mean Squared Error on Training set:\", rmse_train)\n",
    "print(\"Root Mean Squared Error on Testing set:\", rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cef439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b5d466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "966238b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error on Training set: 0.6518409307288845\n",
      "Mean Absolute Percentage Error on Testing set: 7.726569795578484\n",
      "{'rf__max_depth': None, 'rf__max_features': 'log2', 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2, 'rf__n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create Pipeline with feature selection and Random Forest regression\n",
    "pipeline = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(RandomForestRegressor())),\n",
    "    ('rf', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [50, 100, 200, 300],\n",
    "    'rf__max_depth': [None, 5, 10, 20],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4],\n",
    "    'rf__max_features': ['auto', 'sqrt', 'log2']}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "selected_features = X_train.columns[best_model.named_steps['feature_selection'].get_support()]\n",
    "\n",
    "# Evaluate the best model\n",
    "mape_train = np.mean(np.abs((y_train - best_model.predict(X_train)) / y_train)) * 100\n",
    "mape_test = np.mean(np.abs((y_test - best_model.predict(X_test)) / y_test)) * 100\n",
    "\n",
    "print(\"Mean Absolute Percentage Error on Training set:\", mape_train)\n",
    "print(\"Mean Absolute Percentage Error on Testing set:\", mape_test)\n",
    "\n",
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1718d55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Training set: 264.44352124256216\n",
      "Mean Squared Error on Testing set: 40074.72298407441\n",
      "Root Mean Squared Error on Training set: 16.261719504485438\n",
      "Root Mean Squared Error on Testing set: 200.18672029901086\n"
     ]
    }
   ],
   "source": [
    "# Calculate predictions for both training and testing sets\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "# Print MSE and RMSE for both training and testing sets\n",
    "print(\"Mean Squared Error on Training set:\", mse_train)\n",
    "print(\"Mean Squared Error on Testing set:\", mse_test)\n",
    "print(\"Root Mean Squared Error on Training set:\", rmse_train)\n",
    "print(\"Root Mean Squared Error on Testing set:\", rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c2bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02005a07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354815e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28136404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
